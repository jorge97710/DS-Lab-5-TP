library(rpart)
library(caret)
library(tree)
library(rpart.plot)
library(randomForest)
library(MASS)
library(rpart)
library(caTools)
library(party)
library(party)
library(rpart)
library(caret)
library(tree)
library(rpart.plot)
library(randomForest)
library(MASS)
library(rpart)
library(caTools)
library(party)
# Create the input data frame.
input.dat <- todo
# Give the chart file a name.
png(file = "decision_tree.png")
# Create the tree.
output.tree <- ctree(
nativeSpeaker ~ age + shoeSize + score,
data = input.dat)
# Create the input data frame.
input.dat <- todo
# Give the chart file a name.
png(file = "decision_tree.png")
# Create the tree.
output.tree <- ctree(
clasificiacion ~ AÑO + MES + SEXO + GRUPETNICO + EDAD + PERIODOEDA  + Temporada,
data = input.dat)
# Plot the tree.
plot(output.tree)
gc()
# Create the input data frame.
input.dat <- todo
# Give the chart file a name.
png(file = "decision_tree.png")
# Create the tree.
output.tree <- ctree(
clasificiacion ~ AÑO + MES + SEXO + GRUPETNICO + EDAD + PERIODOEDA  + Temporada,
data = input.dat)
# Plot the tree.
plot(output.tree)
gc()
gc()
gc()
gc()
gc()
gc()
Create the input data frame.
input.dat <- todo
# Give the chart file a name.
png(file = "decision_tree.png")
# Create the tree.
output.tree <- ctree(
clasificiacion ~ AÑO + MES + SEXO + GRUPETNICO + EDAD + PERIODOEDA  + Temporada,
data = input.dat)
# Plot the tree.
plot(output.tree)
# Create the input data frame.
input.dat <- todo
gc()
# Give the chart file a name.
png(file = "decision_tree.png")
# Create the tree.
output.tree <- ctree(
clasificiacion ~ AÑO + MES + SEXO + GRUPETNICO + EDAD + PERIODOEDA  + Temporada,
data = input.dat)
gc()
head(todo)
todo <- todo[,-1]
head(todo)
todo <- todo[,-1]
todo <- todo[,-1]
head(todo)
head(todo)
todo <- todo[,-9]
head(todo)
scaleddata<-scale(todo)
scaleddata
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize
maxmindf <- as.data.frame(lapply(todo, normalize))
maxmindf
1830054*.7
# Training and Test Data
trainset <- maxmindf[1:1281038, ]
testset <- maxmindf[1281039:1830054, ]
library(neuralnet)
nn <- neuralnet(Clasifiacion ~Temporada, data=trainset, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
nn <- neuralnet(clasifiacion ~Temporada, data=trainset, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
nn <- neuralnet(clasificiacion ~Temporada, data=trainset, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
plot(nn)
#Test the resulting output
temp_test <- subset(testset, select = c("Temporada"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = testset$clasificiacion, prediction = nn.results$net.result)
nn.results
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(actual,prediction)
roundedresults<-sapply(results,round,digits=3)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
attach(roundedresultsdf)
table(actual,prediction)
predicted=results$prediction * abs(diff(range(clasificiacion))) + min(clasificiacion)
roundedresults<-sapply(results,round,digits=4)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
#Test the resulting output
temp_test <- subset(testset, select = c("Temporada"))
head(temp_test)
temp_test
nn.results <- compute(nn, temp_test)
nn.results
results <- data.frame(actual = testset$clasificiacion, prediction = nn.results$net.result)
results
nn <- neuralnet(clasificiacion ~AÑO + MES + SEXO + GRUPETNICO + EDAD + PERIODOEDA  + Temporada, data=trainset, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
# Read the Data
data = read.csv("estudioenfermedad.csv", header=T)
# Random sampling
samplesize = 0.70 * nrow(data)
set.seed(80)
index = sample( seq_len ( nrow ( data ) ), size = samplesize )
# Create training and test set
datatrain = data[ index, ]
datatest = data[ -index, ]
max = apply(data , 2 , max)
min = apply(data, 2 , min)
scaled = as.data.frame(scale(data, center = min, scale = max - min))
data<- data[-1,]
# Read the Data
data = read.csv("estudioenfermedad.csv", header=T)
data<- data[-1,]
data<- data[-1,]
data<- data[-1,]
# Read the Data
data = read.csv("estudioenfermedad.csv", header=T)
data<- data[,-1]
data<- data[,-1]
data<- data[,-1]
View(data)
data<- data[,-9]
samplesize = 0.70 * nrow(data)
set.seed(80)
index = sample( seq_len ( nrow ( data ) ), size = samplesize )
# Create training and test set
datatrain = data[ index, ]
datatest = data[ -index, ]
max = apply(data , 2 , max)
min = apply(data, 2 , min)
scaled = as.data.frame(scale(data, center = min, scale = max - min))
scaled
# load library
library(neuralnet)
# creating training and test set
trainNN = scaled[index , ]
testNN = scaled[-index , ]
# fit neural network
set.seed(2)
NN = neuralnet(clasificiacion ~ AÑO+MES+SEXO+GRUPETNICO+EDAD+PERIODOEDA+Temporada, trainNN, hidden = 3 , linear.output = T )
NN = neuralnet(clasificiacion ~ AÑO+MES+SEXO+GRUPETNICO+EDAD+PERIODOEDA+Temporada, trainNN, hidden = 3 , linear.output = T,verbose=True)
NN = neuralnet(clasificiacion ~ Temporada, trainNN, hidden = 3 , linear.output = T)
# plot neural network
plot(NN)
NN = neuralnet(clasificiacion ~ AÑO+Temporada, trainNN, hidden = 3 , linear.output = T)
NN = neuralnet(clasificiacion ~ MES+Temporada, trainNN, hidden = 3 , linear.output = T)
# Random sampling
samplesize = 0.95 * nrow(data)
set.seed(80)
index = sample( seq_len ( nrow ( data ) ), size = samplesize )
# Create training and test set
datatrain = data[ index, ]
datatest = data[ -index, ]
max = apply(data , 2 , max)
min = apply(data, 2 , min)
scaled = as.data.frame(scale(data, center = min, scale = max - min))
# creating training and test set
trainNN = scaled[index , ]
testNN = scaled[-index , ]
# fit neural network
set.seed(2)
NN = neuralnet(clasificiacion ~ MES+Temporada, testNN, hidden = 3 , linear.output = T)
todo <- read.csv("estudioenfermedad.csv")
cor(todo$clasificiacion,todo$Temporada,method="spearman")
plot(todo$Temporada,todo$clasificiacion)
prueba = lm(clasificiacion ~ Temporada, data=todo)
summary(prueba)
gc()
library(data.table)
install.packages("data.table")
install.packages("data.table")
library(data.table)
library(data.table)
mydat <- fread('http://www.stats.ox.ac.uk/pub/datasets/csb/ch11b.dat')
head(mydat)
mydat <- fread('https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/')
install.packages("curl")
mydat <- fread('https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/')
head(mydat)
library(data.table)
library(RCurl)
install.packages("RCurl")
myfile <- getURL('https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/
.csv', ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)
myfile <- getURL('https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/.csv', ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)
myfile <- getURL('https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/', ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)
myfile <- getURL('https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/', ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)
library(RCurl)
myfile <- getURL('https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/', ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)
myfile <- getURL('https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/', ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)
myfile <- getURL('https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/')
myfile <- getURL('https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/')
read.table.url("https://www.mineduc.gob.gt/BUSCAESTABLECIMIENTO_GE/",skip=4,header=T)
##prueba imagenes
getwd()
library(imager)
install.packages("imager")
library(imager)
plot(boats)
grayscale(boats)
plot(grayscale(boats))
dim(boats)
plot(boats)
dim(boats)
resize(boats,size_x=10,size_y=10)
plot(resize(boats,size_x=10,size_y=10))
plot(resize(boats,size_x=100,size_y=100))
gr <- imgradient(boats.g,"xy")
gr
plot(gr,layout="row")
gr <- imgradient(boats.g,"xy")
boats
boats.g
gr <- imgradient(boats,"xy")
gr
plot(gr,layout="row")
dx <- imgradient(boats,"x")
dy <- imgradient(boats,"y")
grad.mag <- sqrt(dx^2+dy^2)
plot(grad.mag,main="Gradient magnitude")
imgradient(boats.g,"xy") %>% enorm %>% plot(main="Gradient magnitude (again)")
imgradient(boats,"xy") %>% enorm %>% plot(main="Gradient magnitude (again)")
gr <- imgradient(grayscale(boats),"xy")
gr
plot(gr,layout="row")
dx <- imgradient(grayscale(boats),"x")
dy <- imgradient(grayscale(boats),"y")
grad.mag <- sqrt(dx^2+dy^2)
plot(grad.mag,main="Gradient magnitude")
imgradient(boats,"xy") %>% enorm %>% plot(main="Gradient magnitude (again)")
boats
# Convert the 3D array to a matrix of #rrggbb values
img <-grayscale(boats)
img
img <- as.raster(img)
# Create a count table
tab <- table(img)
# Convert to a data.frame
tab <- data.frame(Color = names(tab), Count = as.integer(tab))
# Extract red/green/blue values
RGB <- t(col2rgb(tab$Color))
tab <- cbind(tab, RGB)
# Preview
head(tab)
##pp
source("http://bioconductor.org/biocLite.R")
biocLite("EBImage")
library("EBImage")
install.packages("EBImage")
as.data.frame(boats)
as.data.frame(boats,wwide = "c")
as.data.frame(boats,wide = "c")
as.data.frame(grayscale(boats),wide = "c")
grayscale(boats)
plot(grayscale(boats))
as.data.frame(grayscale(boats),wide = "c")
plot(grayscale(boats))
imgradient(grayscale(boats),"xy") %>% enorm %>% plot(main="Gradient magnitude (again)")
as.data.frame(imgradient(grayscale(boats),"xy") %>% enorm %>% plot(main="Gradient magnitude (again)")
,wide = "c")
plot(resize(boats,size_x=100,size_y=100))
plot(resize(boats,size_x=10,size_y=10))
plot(resize(grayscale(boats),size_x=10,size_y=10))
imgradient(resize(grayscale(boats),size_x=10,size_y=10),"xy") %>% enorm %>% plot(main="Gradient magnitude (again)")
as.data.frame(imgradient(resize(grayscale(boats),size_x=10,size_y=10),"xy") %>% enorm %>% plot(main="Gradient magnitude (again)")
,wide = "c")
imgradient(resize(grayscale(boats),size_x=2,size_y=2),"xy") %>% enorm %>% plot(main="Gradient magnitude (again)")
as.data.frame(imgradient(resize(grayscale(boats),size_x=2,size_y=2),"xy") %>% enorm %>% plot(main="Gradient magnitude (again)")
,wide = "c")
as.data.frame(imgradient(grayscale(boats),"xy") %>% enorm %>% plot(main="Gradient magnitude (again)")
,wide = "c")
getwd()
fpath <- system.file('c:Users/jazmi/OneDrive/Documentos/original3_figura2.jpg',package='imager')
parrots <- load.image(fpath)
fpath <- system.file('c:Users/jazmi/OneDrive/Documentos/original3_figura2.jpg',package='imager')
parrots <- load.image(fpath)
plot(parrots)
fpath
fpath
fpath <- system.file('c:Users/jazmi/OneDrive/Documentos/original3_figura2.jpg',package='imager')
fpath
fpath <- system.file('c:Users/jazmi/OneDrive/Documentos/original3_figura2.jpg',package='imager')
parrots <- load.image(fpath)
fpath <- system.file('c:Users/jazmi/OneDrive/Documentos/foto.PNG',package='imager')
parrots <- load.image(fpath)
fpath
fpath <- system.file('c:Users/jazmi/OneDrive/Documentos/foto.PNG',package='imager')
install.packages("tm")
getwd()
setwd("C:/Users/jazmi/OneDrive/Documentos/GitHub/DS-Lab-5-TP")
twitterdata <- readLines("en_US.twitter.txt",skipNul = TRUE)
blogdata <- readLines("en_US.blogs.txt", skipNul = TRUE)
noticiasdata <- file("en_US.news.txt", 'rb')
gc()
noticias <- readLines(noticiasdata)  #newsdata
twitterdata <- readLines("en_US.twitter.txt",skipNul = TRUE)
blogdata <- readLines("en_US.blogs.txt", skipNul = TRUE)
noticiasdata <- file("en_US.news.txt", 'rb')
noticias <- readLines(noticiasdata)  #newsdata
close(noticiasdata)
rm(noticiasdata)
library("tm")
library(dplyr)
#limpieza
twitterdata <- Corpus(VectorSource(twitterdata))
twitterdata = tm_map(twitterdata, content_transformer(tolower))
twitterdata = tm_map(twitterdata, removePunctuation)
twitterdata <- tm_map(twitterdata, stripWhitespace)
matrizdocumentacion <- TermDocumentMatrix(twitterdata)
matrizdocumentacion
library(tidyverse)
library(tau)
library(tm)
library(hash)
blog <- readLines("data/en_US/en_US.blogs.txt")
news <- readLines("data/en_US/en_US.news.txt")
twit <- readLines("data/en_US/en_US.twitter.txt")
# batch size
nbatch <- 50
blog.len <- ceiling(length(blog.full) / nbatch)
news.len <- ceiling(length(news.full) / nbatch)
twit.len <- ceiling(length(twit.full) / nbatch)
# preprocessing functions
removeURL <- function(x) gsub("http\\S+", "", x)
removeHash <- function(x) gsub("[@#&]\\S+", "", x)
# don't know how to remove location, like "@ los angeles"
removeNumPunct <- function(x) gsub("[^A-z[:space:]']*", "", x)
# it turns out that Japanese characters belong to [:alpha:], but not [A-z]. Absurd.
h <- hash()
# loop over batches
for (b in 1:nbatch - 1) {
# for (b in 12932) {
message(sprintf("Processing the %i-th batch", b))
# concatenate text and preprocess
blog <- blog.full[blog.len * b + (1:blog.len)]
news <- news.full[news.len * b + (1:news.len)]
twit <- twit.full[twit.len * b + (1:twit.len)]
bnt <- c(blog, news, twit) %>%
removeURL() %>% removeHash() %>%
removeNumPunct() %>% tolower() %>% stripWhitespace()
# obtain trigram
trigram <- textcnt(bnt, n=3, split=" ", method="string", decreasing = TRUE)
# create hash table for fast prediction
for (i in 1:length(trigram)) {
if (trigram[i] < 4) {
break
} else {
gram <- strsplit(names(trigram[i]), split = ' ')[[1]]
history <- paste0(gram[1:2], collapse = ' ')
candidate <- gram[3]
count <- trigram[[i]]
if (candidate %in% h[[history]]$candidate) {
index <- h[[history]]$candidate == candidate
h[[history]]$count[index] <- h[[history]]$count[index] + count
} else {
h[[history]]$candidate <- c(h[[history]]$candidate, candidate)
h[[history]]$count <- c(h[[history]]$count, count)
}
}
}
gc()
}
library(tidyverse)
library(tau)
library(tm)
library(hash)
blog <- readLines("en_US/en_US.blogs.txt")
news <- readLines("en_US/en_US.news.txt")
twit <- readLines("en_US/en_US.twitter.txt")
# batch size
nbatch <- 50
blog.len <- ceiling(length(blog.full) / nbatch)
news.len <- ceiling(length(news.full) / nbatch)
twit.len <- ceiling(length(twit.full) / nbatch)
# preprocessing functions
removeURL <- function(x) gsub("http\\S+", "", x)
removeHash <- function(x) gsub("[@#&]\\S+", "", x)
# don't know how to remove location, like "@ los angeles"
removeNumPunct <- function(x) gsub("[^A-z[:space:]']*", "", x)
# it turns out that Japanese characters belong to [:alpha:], but not [A-z]. Absurd.
h <- hash()
# loop over batches
for (b in 1:nbatch - 1) {
# for (b in 12932) {
message(sprintf("Processing the %i-th batch", b))
# concatenate text and preprocess
blog <- blog.full[blog.len * b + (1:blog.len)]
news <- news.full[news.len * b + (1:news.len)]
twit <- twit.full[twit.len * b + (1:twit.len)]
bnt <- c(blog, news, twit) %>%
removeURL() %>% removeHash() %>%
removeNumPunct() %>% tolower() %>% stripWhitespace()
# obtain trigram
trigram <- textcnt(bnt, n=3, split=" ", method="string", decreasing = TRUE)
# create hash table for fast prediction
for (i in 1:length(trigram)) {
if (trigram[i] < 4) {
break
} else {
gram <- strsplit(names(trigram[i]), split = ' ')[[1]]
history <- paste0(gram[1:2], collapse = ' ')
candidate <- gram[3]
count <- trigram[[i]]
if (candidate %in% h[[history]]$candidate) {
index <- h[[history]]$candidate == candidate
h[[history]]$count[index] <- h[[history]]$count[index] + count
} else {
h[[history]]$candidate <- c(h[[history]]$candidate, candidate)
h[[history]]$count <- c(h[[history]]$count, count)
}
}
}
gc()
}
library(tidyverse)
library(tau)
library(tm)
library(hash)
install.packages(tau)
install.packages("tau")
install.packages("hash")
library(tidyverse)
library(tau)
library(tm)
library(hash)
blog <- readLines("en_US/en_US.blogs.txt")
blog <- readLines("en_US.blogs.txt")
blog.full <- readLines("en_US.blogs.txt")
news.full <- readLines("en_US.news.txt")
twit.full <- readLines("en_US.twitter.txt")
# batch size
nbatch <- 50
blog.len <- ceiling(length(blog.full) / nbatch)
news.len <- ceiling(length(news.full) / nbatch)
twit.len <- ceiling(length(twit.full) / nbatch)
# preprocessing functions
removeURL <- function(x) gsub("http\\S+", "", x)
removeHash <- function(x) gsub("[@#&]\\S+", "", x)
# don't know how to remove location, like "@ los angeles"
removeNumPunct <- function(x) gsub("[^A-z[:space:]']*", "", x)
# it turns out that Japanese characters belong to [:alpha:], but not [A-z]. Absurd.
h <- hash()
# loop over batches
for (b in 1:nbatch - 1) {
# for (b in 12932) {
message(sprintf("Processing the %i-th batch", b))
# concatenate text and preprocess
blog <- blog.full[blog.len * b + (1:blog.len)]
news <- news.full[news.len * b + (1:news.len)]
twit <- twit.full[twit.len * b + (1:twit.len)]
bnt <- c(blog, news, twit) %>%
removeURL() %>% removeHash() %>%
removeNumPunct() %>% tolower() %>% stripWhitespace()
# obtain trigram
trigram <- textcnt(bnt, n=3, split=" ", method="string", decreasing = TRUE)
# create hash table for fast prediction
for (i in 1:length(trigram)) {
if (trigram[i] < 4) {
break
} else {
gram <- strsplit(names(trigram[i]), split = ' ')[[1]]
history <- paste0(gram[1:2], collapse = ' ')
candidate <- gram[3]
count <- trigram[[i]]
if (candidate %in% h[[history]]$candidate) {
index <- h[[history]]$candidate == candidate
h[[history]]$count[index] <- h[[history]]$count[index] + count
} else {
h[[history]]$candidate <- c(h[[history]]$candidate, candidate)
h[[history]]$count <- c(h[[history]]$count, count)
}
}
}
gc()
}
h
h[["how are"]]
h[["I am"]]
h[["i am"]]
gc()
